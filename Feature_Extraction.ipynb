{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports you will need in the whole lab\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage.transform import (hough_line, hough_line_peaks)\n",
    "from skimage.feature import greycomatrix, greycoprops, canny, corner_harris\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing,skeletonize, thin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.filters import sobel_h, sobel, sobel_v,roberts, prewitt,threshold_otsu\n",
    "import glob\n",
    "import math\n",
    "from matplotlib import cm\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles=None):\n",
    "    # This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None:\n",
    "        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        a = fig.add_subplot(1, n_ims, n)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = np.asarray(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(gray):\n",
    "\n",
    "\n",
    "    img = cv2.GaussianBlur(gray,(3,3),0)\n",
    "\n",
    "    # convolute with proper kernels\n",
    "    dest = cv2.Laplacian(gray, cv2.CV_16S, ksize=3)\n",
    "    abs_dest = cv2.convertScaleAbs(dest)\n",
    "   \n",
    "    Otsu_Threshold = threshold_otsu(gray)   \n",
    "    binary = gray < Otsu_Threshold  \n",
    "    skeleton_img =  skeletonize(binary)\n",
    "\n",
    "    #separating the diac.\n",
    "    H = np.sum(binary,axis = 1)\n",
    "    I = np.argmax(H)\n",
    "    point = [(x,I) for x in range(binary.shape[0])]\n",
    "    # print(I)\n",
    "    h, w = binary.shape[:2]\n",
    "    # print(h,w)\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    # print(mask)\n",
    "    binary = binary < 1 \n",
    "    binary = binary.astype(np.uint8)\n",
    "    # show_images([binary])\n",
    "    for j in range(binary.shape[1] - 1):\n",
    "        if binary[I][j] == 0 and binary[I][j+1] == 1:\n",
    "            cv2.floodFill(binary,mask,seedPoint = (j,I),newVal = 1) \n",
    "    # print(b)\n",
    "    text = mask\n",
    "    diacritics = binary\n",
    "    return img,abs_dest,skeleton_img,text,diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HVSL(edge):\n",
    "    horizontal = edge.copy()\n",
    "    vertical = edge.copy()\n",
    "    H = 0\n",
    "    V = 0 \n",
    "    \n",
    "    cols = horizontal.shape[1]\n",
    "    horizontal_size = cols / 30\n",
    "    horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (int(horizontal_size + 4), 1))\n",
    "    horizontal = cv2.erode(horizontal, horizontalStructure)\n",
    "    horizontal = cv2.dilate(horizontal, horizontalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(horizontal)   \n",
    "    horizontal = horizontal > Otsu_Threshold \n",
    "    H, output, stats, centroids = cv2.connectedComponentsWithStats(horizontal.astype(np.uint8),connectivity=8)\n",
    "    \n",
    "\n",
    "\n",
    "    rows = vertical.shape[0]\n",
    "    verticalsize = rows / 30\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1,int(verticalsize + 4)))\n",
    "    vertical = cv2.erode(vertical, verticalStructure)\n",
    "    vertical = cv2.dilate(vertical.astype(np.uint8), verticalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(vertical)   \n",
    "    vertical = vertical > Otsu_Threshold \n",
    "\n",
    "    V, output, stats, centroids = cv2.connectedComponentsWithStats(vertical.astype(np.uint8),connectivity=8)\n",
    "\n",
    "    \n",
    "    # for i in range(1,horizontal.shape[0] ):    \n",
    "    #     for j in range(horizontal.shape[1]):\n",
    "    #         if horizontal[i][j] == 1 and horizontal[i - 1 ][j] == 0:\n",
    "    #             H += 1\n",
    "\n",
    "    \n",
    "\n",
    "    # vertical_z = binary_erosion(vertical,[[1,1,1,1],[0,0,0,0],[0,0,0,0],[0,0,0,0]])\n",
    "    # print(horizontal,vertical)\n",
    "\n",
    "    # for i in range(vertical.shape[0] - 1):    \n",
    "    #     for j in range(1,vertical.shape[1] - 1):\n",
    "            \n",
    "    #         if vertical[i][j] == 1 and vertical[i][j-1] == 0:\n",
    "    #             V += 1\n",
    "    # sizes = stats[1:, -1]\n",
    "    # V = V - 1\n",
    "\n",
    "    # # your answer image\n",
    "    # img2 = vertical\n",
    "    # # for every component in the image, you keep it only if it's above min_size\n",
    "    # for i in range(0, V):\n",
    "    #     if sizes[i] < 20:\n",
    "    #         img2[output == i + 1] = 0\n",
    "\n",
    "    # houghSpace,angles, dists = hough_line(horizontal)\n",
    "    # houghSpace,angles, dists = hough_line_peaks(houghSpace,angles,dists,threshold=0.5*np.amax(houghSpace))\n",
    "    # H = 0\n",
    "    # V = 0\n",
    "    # for angle, dist in zip(angles,dists):\n",
    "    #     if(angle == 0 or angle == math.pi):\n",
    "    #         H += 1\n",
    "    #     elif(angle == math.pi/2 or angle == 1.5 * math.pi):\n",
    "    #         V += 1\n",
    "    return H - 1,V - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOE(edge):\n",
    "    houghSpace,angles, dists = hough_line(edge)\n",
    "    houghSpace,angles, dists = hough_line_peaks(houghSpace,angles,dists,threshold=0.5*np.amax(houghSpace))\n",
    "    start = -2\n",
    "    bin = []\n",
    "    while True :\n",
    "        if(start >= 2):\n",
    "            break\n",
    "        bin.append(start)\n",
    "        start += 0.01\n",
    "    bin = [round(bins,2) for bins in bin ]\n",
    "    angles = [round(angle,2) for angle in angles ]\n",
    "\n",
    "    hist , bins = np.histogram(angles,bins = bin)\n",
    "    return hist,bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOS(skeleton):\n",
    "    houghSpace,angles, dists = hough_line(skeleton)\n",
    "    houghSpace,angles, dists = hough_line_peaks(houghSpace,angles,dists,threshold=0.5*np.amax(houghSpace))\n",
    "\n",
    "    start = -2\n",
    "    bin = []\n",
    "    while True :\n",
    "        if(start >= 2):\n",
    "            break\n",
    "        bin.append(start)\n",
    "        start += 0.01\n",
    "    bin = [round(bins,2) for bins in bin ]\n",
    "    angles = [round(angle,2) for angle in angles ]\n",
    "    hist , bins = np.histogram(angles,bins = bin)\n",
    "    return hist,bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVL(ske):\n",
    "    vertical = ske.copy()\n",
    "    V = 0 \n",
    "    rows = vertical.shape[0]\n",
    "    verticalsize = rows // 30\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, int(verticalsize + 4)))\n",
    "    vertical = cv2.erode(vertical.astype(np.uint8), verticalStructure)\n",
    "    vertical = cv2.dilate(vertical.astype(np.uint8), verticalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(vertical)   \n",
    "    vertical = vertical > Otsu_Threshold \n",
    "    V, output, stats, centroids = cv2.connectedComponentsWithStats(vertical.astype(np.uint8),connectivity=8)\n",
    "    sizes = stats[1:, -1]\n",
    "\n",
    "    Otsu_Threshold = threshold_otsu(ske)   \n",
    "    ske = ske > Otsu_Threshold \n",
    "    min = math.inf\n",
    "    max = -1\n",
    "    for i in range(ske.shape[0]):\n",
    "        for j in range(ske.shape[1]):\n",
    "            if (ske[i][j] == 1):\n",
    "                if i > max:\n",
    "                    max = i\n",
    "                elif i < min:\n",
    "                    min = i\n",
    "    text_hight = max - min\n",
    "    num_VL = V - 1\n",
    "    if(len(sizes) == 0):\n",
    "        sizes = [2,1,2]\n",
    "    higtest_VL = np.max(sizes)\n",
    "    drvt = text_hight - higtest_VL\n",
    "    variance = np.var(sizes)\n",
    "    # print(sizes)\n",
    "    # print(text_hight,\"text hight\")\n",
    "    # print(num_VL,\"num_VL\")\n",
    "    # print(higtest_VL,\"higtest_VL\")\n",
    "    # print(drvt,\"drvt\")\n",
    "    # print(variance,\"variance\")\n",
    "    return text_hight,num_VL,higtest_VL,drvt,variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tth(edge,ske):\n",
    "    Otsu_Threshold = threshold_otsu(ske)   \n",
    "    ske = ske > Otsu_Threshold \n",
    "\n",
    "    Otsu_Threshold = threshold_otsu(edge)   \n",
    "    edge = edge > Otsu_Threshold \n",
    "\n",
    "    dest_up = []\n",
    "    dest_down = []\n",
    "    for i in range(ske.shape[0]):\n",
    "        for j in range(ske.shape[1]):\n",
    "            if (ske[i][j] == 1):\n",
    "                c = True\n",
    "                for k in range(i + 1,edge.shape[0]):\n",
    "                    if(edge[k][j] == 1):\n",
    "                        c = False\n",
    "                        d = abs(k - i)\n",
    "                        dest_down.append(d)\n",
    "                        break\n",
    "                if(c):\n",
    "                    dest_down.append(0)        \n",
    "                \n",
    "                u = 0\n",
    "                for k in range(0, i):\n",
    "                    if(edge[k][j] == 1):\n",
    "                        u = k\n",
    "                dest_up.append(abs(u-i))\n",
    "    return dest_down + dest_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDs(diacritics):\n",
    "\n",
    "    arra  = np.zeros(diacritics.shape)\n",
    "    arra = diacritics == 0\n",
    "\n",
    "    contours = find_contours(arra ,level = 0.2,fully_connected='high')\n",
    "\n",
    "    \n",
    "    m1 = rgb2gray(io.imread(\"mark1.jpg\"))\n",
    "    Otsu_Threshold = threshold_otsu(m1)   \n",
    "    m1 = m1 < Otsu_Threshold \n",
    "\n",
    "\n",
    "    m2 = rgb2gray(io.imread(\"mark2.jpg\"))\n",
    "    Otsu_Threshold = threshold_otsu(m2)   \n",
    "    m2 = m2 < Otsu_Threshold \n",
    "\n",
    "    dist_1 = []\n",
    "    dist_2 = []\n",
    "    bounding_boxes = []\n",
    "    for contour in contours:\n",
    "        Y_Values = np.asarray(contour[:,0])\n",
    "        X_Values = np.asarray(contour[:,1])\n",
    "        bounding_boxes.append([\n",
    "        int(np.amin(X_Values)),\n",
    "        int(np.amax(X_Values)),\n",
    "        int(np.amin(Y_Values)),\n",
    "        int(np.amax(Y_Values))])\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        [Xmin, Xmax, Ymin, Ymax] = box\n",
    "        dist_1.append(cv2.matchShapes(arra[Ymin:Ymax,Xmin:Xmax].astype(np.uint8),m1.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0))\n",
    "        dist_2.append(cv2.matchShapes(arra[Ymin:Ymax,Xmin:Xmax].astype(np.uint8),m2.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0))\n",
    "    if (len(dist_1) == 0 ):\n",
    "        return 0 , 0 \n",
    "    return np.min(dist_1),np.min(dist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOR(text):\n",
    "    contours, _ = cv2.findContours(text.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    angles = []\n",
    "    for i, c in enumerate(contours):\n",
    "        \n",
    "        # Calculate the area of each contour\n",
    "        area = cv2.contourArea(c)\n",
    "        \n",
    "        # Ignore contours that are too small or too large\n",
    "        rect = cv2.minAreaRect(c)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "        center = (int(rect[0][0]),int(rect[0][1])) \n",
    "        width = int(rect[1][0])\n",
    "        height = int(rect[1][1])\n",
    "        angle = int(rect[2])\n",
    "\n",
    "        if width < height:\n",
    "            angle = 90 - angle\n",
    "        else:\n",
    "            angle = -angle\n",
    "        angles.append(angle)\n",
    "    angles = np.sort(angles)[:-1]\n",
    "    ori = np.average(angles)\n",
    "    if(len(angles) == 0):\n",
    "        return 0\n",
    "    return(ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPP(img):\n",
    "    H = np.sum(img[1:img.shape[0]-1,:],axis = 1)\n",
    "    I = np.argmax(H)\n",
    "    H = np.sort(H)\n",
    "    if(I == 0):\n",
    "        I = 1\n",
    "    hpp = np.sum(img[:,1:img.shape[1] - 1]) / I\n",
    "    return H[-3:],hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img,edges,ske,text,diacritics = preprocessing(\"ACdata_base/9/1510.JPG\")\n",
    "# Otsu_Threshold = threshold_otsu(img)   \n",
    "# img = img < Otsu_Threshold\n",
    "# show_images([img,edges,ske,text,diacritics])\n",
    "# H,V = HVSL(edges)\n",
    "# hist_e,b_e = TOE(edges)\n",
    "# hist_s,b_s = TOS(ske)\n",
    "# LVL(ske)\n",
    "# Tth(edges,ske)\n",
    "# SDs(diacritics)\n",
    "# HPP(text)\n",
    "# WOR(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    xData = []\n",
    "    yData = []\n",
    "    for filename in sorted(glob.glob('ACdata_base/1/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(1)\n",
    "    for filename in sorted(glob.glob('ACdata_base/2/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(2)\n",
    "    for filename in sorted(glob.glob('ACdata_base/3/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(3)\n",
    "    for filename in sorted(glob.glob('ACdata_base/4/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(4)\n",
    "    for filename in sorted(glob.glob('ACdata_base/5/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(5)\n",
    "    for filename in sorted(glob.glob('ACdata_base/6/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(6)\n",
    "    for filename in sorted(glob.glob('ACdata_base/7/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(7)\n",
    "    for filename in sorted(glob.glob('ACdata_base/8/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(8)\n",
    "    for filename in sorted(glob.glob('ACdata_base/9/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(9)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(np.asarray(xData), np.asarray(yData), test_size = 0.2, shuffle = True)\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featur_Extraction(xTrain):\n",
    "    Features = []\n",
    "    for gray in xTrain:\n",
    "        img_feature = []\n",
    "        img,edges,ske,text,diacritics = preprocessing(gray)\n",
    "        Otsu_Threshold = threshold_otsu(img)   \n",
    "        img = img < Otsu_Threshold\n",
    "\n",
    "        H,V = HVSL(edges)\n",
    "        img_feature.append(H)\n",
    "        img_feature.append(V)\n",
    "\n",
    "        hist_e , b_e = TOE(edges)\n",
    "        hist_s , b_s = TOS(ske)\n",
    "        for i in range(len(hist_e)):\n",
    "            img_feature.append(hist_e[i])\n",
    "            img_feature.append(hist_s[i])\n",
    "        \n",
    "        text_hight,num_VL,higtest_VL,drvt,variance = LVL(ske)\n",
    "        img_feature.append(text_hight)\n",
    "        img_feature.append(num_VL)\n",
    "        img_feature.append(higtest_VL)\n",
    "        img_feature.append(drvt)\n",
    "        img_feature.append(variance)\n",
    "\n",
    "        Thickness = Tth(edges,ske)\n",
    "        for i in range(200):\n",
    "            img_feature.append(Thickness[i])\n",
    "        \n",
    "        d1 , d2 = SDs(diacritics)\n",
    "        img_feature.append(d1)\n",
    "        img_feature.append(d2)\n",
    "        \n",
    "        H , hpp = HPP(text)\n",
    "        img_feature.append(H[0])\n",
    "        img_feature.append(H[1])\n",
    "        img_feature.append(H[2])\n",
    "        img_feature.append(hpp)\n",
    "         \n",
    "        ori = WOR(text)\n",
    "        img_feature.append(ori)\n",
    "        Features.append(img_feature)\n",
    "        \n",
    "    return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp/ipykernel_16092/2948043143.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  xTrain, xTest, yTrain, yTest = train_test_split(np.asarray(xData), np.asarray(yData), test_size = 0.2, shuffle = True)\n",
      "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = readData()\n",
    "xTrain = Featur_Extraction(xTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    Data = np.genfromtxt(file_name,delimiter = \",\")\n",
    "    ## HINT 1: How is the data ordered in the file?\n",
    "    ## HINT 2: Do you need to cast the data you read from the file?\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featur_Extraction_test(gray):\n",
    "    img_feature = []\n",
    "    img,edges,ske,text,diacritics = preprocessing(gray)\n",
    "    Otsu_Threshold = threshold_otsu(img)   \n",
    "    img = img < Otsu_Threshold\n",
    "\n",
    "    H,V = HVSL(edges)\n",
    "    img_feature.append(H)\n",
    "    img_feature.append(V)\n",
    "\n",
    "    hist_e , b_e = TOE(edges)\n",
    "    hist_s , b_s = TOS(ske)\n",
    "    for i in range(len(hist_e)):\n",
    "        img_feature.append(hist_e[i])\n",
    "        img_feature.append(hist_s[i])\n",
    "    \n",
    "    text_hight,num_VL,higtest_VL,drvt,variance = LVL(ske)\n",
    "    img_feature.append(text_hight)\n",
    "    img_feature.append(num_VL)\n",
    "    img_feature.append(higtest_VL)\n",
    "    img_feature.append(drvt)\n",
    "    img_feature.append(variance)\n",
    "\n",
    "    Thickness = Tth(edges,ske)\n",
    "    for i in range(200):\n",
    "        img_feature.append(Thickness[i])\n",
    "    \n",
    "    d1 , d2 = SDs(diacritics)\n",
    "    img_feature.append(d1)\n",
    "    img_feature.append(d2)\n",
    "    \n",
    "    H , hpp = HPP(text)\n",
    "    # diff = abs((H[0] - H[1]) + (H[1] - H[2]))\n",
    "    img_feature.append(H[0])\n",
    "    img_feature.append(H[1])\n",
    "    img_feature.append(H[2])\n",
    "    img_feature.append(hpp)\n",
    "        \n",
    "    ori = WOR(text)\n",
    "    img_feature.append(ori)\n",
    "    return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(x1, x2):\n",
    "    distance = np.linalg.norm(x1 - x2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinimumDistanceClassifier(testPoint, trainingFeatures, yTrain):\n",
    "    distances = list()\n",
    "    centers = [np.mean(trainingFeatures[yTrain == 1], axis=0), np.mean(trainingFeatures[yTrain == 2], axis=0), np.mean(trainingFeatures[yTrain == 3], axis=0), np.mean(trainingFeatures[yTrain == 4], axis=0), np.mean(trainingFeatures[yTrain == 5], axis=0), np.mean(trainingFeatures[yTrain == 6], axis=0), np.mean(trainingFeatures[yTrain == 7], axis=0), np.mean(trainingFeatures[yTrain == 8], axis=0), np.mean(trainingFeatures[yTrain == 9], axis=0)]\n",
    "    for i in range(len(centers)):\n",
    "        distances.append(calculateDistance(testPoint, centers[i]))\n",
    "    classification = np.argmin(distances)\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348, 1012)\n",
      "(1348,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp/ipykernel_16092/3421315254.py:33: RuntimeWarning: overflow encountered in ulong_scalars\n",
      "  diff = abs((H[0] - H[1]) + (H[1] - H[2]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "Minimum Distance Classifier Accuracy:  60.23738872403561 %\n"
     ]
    }
   ],
   "source": [
    "minDistPredictions = np.zeros(yTest.shape)\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "print(np.asarray(xTrain).astype(np.uint8).shape)\n",
    "print(yTrain.shape)\n",
    "for i in range(len(xTest)):\n",
    "    testPoint = Featur_Extraction_test(xTest[i])\n",
    "    # print(np.asarray(testPoint).astype(np.uint8))\n",
    "    # fig = plt.figure()\n",
    "    # plt.imshow(xTest[i])\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    # print(\"Actual class :\", yTest[i])\n",
    "    # print(\"---------------------------------------\")\n",
    "    minDistPrediction = MinimumDistanceClassifier(np.asarray(testPoint).astype(np.uint8), np.asarray(xTrain).astype(np.uint8), yTrain)\n",
    "    minDistPredictions[i] = classes[minDistPrediction]\n",
    "    # print(\"Minimum Distance Classifier Prediction   :\", classes[minDistPrediction])\n",
    "    # print(\"===========================================================================\")\n",
    "print(np.sum(minDistPredictions == yTest))\n",
    "mdcAccuracy = (np.sum(minDistPredictions == yTest) / len(yTest)) * 100\n",
    "print(\"Minimum Distance Classifier Accuracy: \", mdcAccuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numClasses = 9\n",
    "# M = len(xTrain)\n",
    "# K = len(xTest)\n",
    "# N = len(xTrain[0])\n",
    "\n",
    "# X = np.asarray(xTrain)\n",
    "# X_Test = xTest\n",
    "# Y = np.uint8(np.asarray(yTrain))\n",
    "\n",
    "xTrain = np.asarray(xTrain)\n",
    "xTest = np.asarray(xTest)\n",
    "numClasses = 9\n",
    "M = np.asarray(xTrain).shape[0]\n",
    "N = np.asarray(xTrain).shape[1] \n",
    "K = np.asarray(xTest).shape[0]\n",
    "\n",
    "X = np.asarray(xTrain)[:, 1:]\n",
    "X_Test = xTest[:]\n",
    "Y = np.reshape(yTrain, (M, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pClasses = [] # A list of size (numClasses, 1) containing the a priori probabilities of each class in the training set.\n",
    "\n",
    "# estimate_means = [] # A numpy array of size (numClasses, N) containing the mean points of each class in the training set. \n",
    "#                     # HINT: USE NP.MEAN\n",
    "\n",
    "# estimate_covariances = [] # A numpy array of size (numClasses, N, N) containing the covariance matrices of each class in the training set.\n",
    "#                           # HINT: USE NP.COV (Pay attenention for what it takes as an argument)\n",
    "# # print(Y)  \n",
    "# for classIndex in range(numClasses):\n",
    "#     # TODO [5]: Estimate the parameters of the Gaussian distributions of the given classes.\n",
    "#     # Fill pClasses, estimate_means, and estimate_covariances in this part \n",
    "#     # Your code should be vectorized WITHOUT USING A SINGLE FOR LOOP.\n",
    "#     # print(len(Y == classIndex + 1),len(X))\n",
    "#     pClasses.append(len(X[Y == classIndex + 1])/len(X))\n",
    "#     estimate_means.append(np.mean(X[Y == classIndex + 1],axis= 0))\n",
    "#     # find out covariance with respect  columns\n",
    "#     # print(X[Y == classIndex + 1].shape)\n",
    "#     cov_mat = np.stack(X[Y == classIndex + 1], axis = 0)\n",
    "#     # print(cov_mat.shape)\n",
    "#     arr = []\n",
    "#     for i in range(len(X)):\n",
    "#         if(i == 1010):\n",
    "#             break\n",
    "#         arr.append(X[Y == classIndex + 1][:,i])\n",
    "        \n",
    "#     # estimate_covariances.append(np.cov(np.asarray(cov_mat).T))\n",
    "#     estimate_covariances.append(np.cov(X[Y == classIndex + 1][:, :].T))\n",
    "#     pass\n",
    "# estimate_means = np.array(estimate_means)\n",
    "# estimate_covariances = np.array(estimate_covariances)\n",
    "\n",
    "pClasses = [] # A list of size (numClasses, 1) containing the a priori probabilities of each class in the training set.\n",
    "\n",
    "estimate_means = [] # A numpy array of size (numClasses, N) containing the mean points of each class in the training set. \n",
    "                    # HINT: USE NP.MEAN\n",
    "\n",
    "estimate_covariances = [] # A numpy array of size (numClasses, N, N) containing the covariance matrices of each class in the training set.\n",
    "                          # HINT: USE NP.COV (Pay attenention for what it takes as an argument)\n",
    "                             \n",
    "for classIndex in range(numClasses):\n",
    "    # TODO [5]: Estimate the parameters of the Gaussian distributions of the given classes.\n",
    "    # Fill pClasses, estimate_means, and estimate_covariances in this part \n",
    "    # Your code should be vectorized WITHOUT USING A SINGLE FOR LOOP.\n",
    "    _, pClasses = np.unique(xTrain[:, 0], return_counts=True)\n",
    "    pClasses = pClasses / M\n",
    "    estimate_means.append(np.mean(xTrain[xTrain[:, 0] == (classIndex + 1)][:, 1:], axis=0))\n",
    "    estimate_covariances.append(np.cov(xTrain[xTrain[:, 0] == (classIndex + 1)][:, 1:].T))\n",
    "\n",
    "estimate_means = np.array(estimate_means)\n",
    "estimate_covariances = np.array(estimate_covariances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16092/3995942497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimate_covariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "print(np.linalg.inv(estimate_covariances[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Implement the multivariate normal gaussian distribution with parameters mu and sigma, and return the\n",
    "#  value in prob.\n",
    "# HINT: Calculate each part of the equation first, then combine them. \n",
    "#      That is: calulate co-efficient first, the parameter of the exponentiation, then combine them\n",
    "def multivariate_normal_gaussian(X, mu, sigma):\n",
    "    coefficient = 1 / (((2 * np.pi) ** (X.shape[0] / 2)) * (np.linalg.det(sigma) ** 0.5))\n",
    "    expParameter = -0.5 * np.matmul(np.matmul((X - mu).T, np.linalg.inv(sigma)), (X - mu))\n",
    "    prob = coefficient * np.exp(expParameter)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16092/3878703274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#  Fill the array classProbabilities accordingly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mclassProbabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultivariate_normal_gaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestPoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mestimate_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mestimate_covariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# TODO [7.B]: Find the prediction of the test point X_Test[i] and append it to the predicted_classes array.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16092/3679103939.py\u001b[0m in \u001b[0;36mmultivariate_normal_gaussian\u001b[1;34m(X, mu, sigma)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#      That is: calulate co-efficient first, the parameter of the exponentiation, then combine them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultivariate_normal_gaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcoefficient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mexpParameter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoefficient\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpParameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "# TODO [7]: Apply the Bayesian Classifier to predict the classes of the test points.\n",
    "predicted_classes = [] # predicted_classes: A numpy array of size (K, 1) where K is the number of points in the test set. Every element in this array\n",
    "                       # contains the predicted class of Bayes classifier for this test point.\n",
    "\n",
    "for i in range(X_Test.shape[0]):\n",
    "    testPoint = np.asarray(Featur_Extraction_test(X_Test[i]))\n",
    "    classProbabilities = np.zeros(numClasses)\n",
    "    # TODO [7.A]: Compute the probability that the test point X_Test[i] belongs to each class in numClasses.\n",
    "    #  Fill the array classProbabilities accordingly.\n",
    "    for j in range(numClasses):\n",
    "        classProbabilities[j] = multivariate_normal_gaussian(testPoint,estimate_means[j],estimate_covariances[j])\n",
    "\n",
    "    # TODO [7.B]: Find the prediction of the test point X_Test[i] and append it to the predicted_classes array.\n",
    "    predicted_classes.append(np.argmax(classProbabilities) + 1) \n",
    "# accuracy = np.count_nonzero(test_data_true[test_data_true == predicted_classes]) / K\n",
    "BayesianClassifierAccuracy = (np.sum(predicted_classes == yTest) / len(yTest)) * 100\n",
    "print(\"Bayesian Classifier Accuracy: \", BayesianClassifierAccuracy, \"%\")\n",
    "# print('Accuracy = ' + str(round(BayesianClassifierAccuracy,4) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16092/2538371603.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "xTest = np.asarray(xTest)\n",
    "xTrain = np.asarray(np.float32(xTrain))\n",
    "yTest = np.asarray(yTest)\n",
    "yTrain = np.asarray(np.float32(yTrain))\n",
    "\n",
    "# for i in range(xTest.shape[0]):\n",
    "#     testPoint = np.asarray(Featur_Extraction_test(xTest[i]))\n",
    "#     x_test.append(testPoint)\n",
    "# x_test = np.asarray(x_test)\n",
    "\n",
    "for i in range(xTrain.shape[0]):\n",
    "    for j in range(xTrain.shape[1]):\n",
    "        if(xTrain[i][j] > 10000 ):\n",
    "            xTrain[i][j] = 0\n",
    "print(np.all(np.isfinite(xTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp/ipykernel_16092/2948043143.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  xTrain, xTest, yTrain, yTest = train_test_split(np.asarray(xData), np.asarray(yData), test_size = 0.2, shuffle = True)\n"
     ]
    }
   ],
   "source": [
    "_, xTest, _, yTest = readData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "for i in range(xTest.shape[0]):\n",
    "    testPoint = np.asarray(Featur_Extraction_test(xTest[i]))\n",
    "    x_test.append(testPoint)\n",
    "x_test = np.asarray(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(337, 1012)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    for j in range(x_test.shape[1]):\n",
    "        if(x_test[i][j] > 10000 ):\n",
    "            x_test[i][j] = 0\n",
    "print(np.all(np.isfinite(x_test)))\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(337, 1012)\n"
     ]
    }
   ],
   "source": [
    "xTest = np.asarray(x_test) \n",
    "print(np.all(np.isfinite(xTest)))\n",
    "print(xTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf):\n",
    "    \n",
    "#     #TODO: FILL THE FUNCTION with the implementation as the steps above\n",
    "\n",
    "#     # TODO [1]: Initialize weights\n",
    "#     w =np.full(X_train.shape[0] , 1 / X_train.shape[0]) \n",
    "    \n",
    "#     ## TODO [2]:  Initialize the training and test data with empty array placeholders\n",
    "#     #### Hint: what should be their shape?\n",
    "    \n",
    "#     pred_train = np.zeros(Y_train.shape) ## predicted classes of the training examples\n",
    "#     pred_test = np.zeros(Y_test.shape)  ## predicted classes of the test examples\n",
    "\n",
    "#     ## TODO [3]: loop over the boosting iterations \n",
    "#     for i in range(T): \n",
    "\n",
    "#         # TODO [4]: Fit a classifier with the specific weights \n",
    "#         ## TODO [4.A]: fit the classifier on the training data\n",
    "#         #### Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\n",
    "#         ### Hint: search for parameter weights in the fit matrix\n",
    "#         clf.fit(X_train , Y_train, w)\n",
    "#         # TODO [4.B]: predict classes for the training data and test data\n",
    "#         pred_train_i = clf.predict(X_train) \n",
    "#         pred_test_i = clf.predict(X_test) \n",
    "#         # TODO [5]: calculate the miss Indicator function\n",
    "#         m_I = (Y_train!= pred_train_i).astype(int)\n",
    "#         # TODO [6]: calculate the error for the current classifier (err_t)\n",
    "#         err_t = ( np.sum(w * m_I) ) / (np.sum(w)) \n",
    "        \n",
    "#         # TODO [7]: calculate current classifier weight (Alpha_t)\n",
    "#         alpha_t = np.log((1 - err_t) / err_t)\n",
    "        \n",
    "#         # TODO [8]: update the weights \n",
    "#         w = w * np.exp(alpha_t * m_I)\n",
    "#         w = w / np.sum(w)\n",
    "#         # TODO [9] Add to the overall predictions\n",
    "#         pred_train += alpha_t * pred_train_i\n",
    "#         pred_test += alpha_t * pred_test_i\n",
    "        \n",
    "#     pred_train = np.sign(pred_train)\n",
    "#     pred_test = np.sign(pred_test)\n",
    "#     print(pred_train)\n",
    "#     # TODO [10]: Return error rate in train and test set\n",
    "#     #### Hint: use function get_accuracy from utils.py\n",
    "#     BayesianClassifierAccuracy = (np.sum(pred_test == Y_test) / len(Y_test)) * 100\n",
    "#     print(\"Bayesian Classifier Accuracy: \", BayesianClassifierAccuracy, \"%\")\n",
    "\n",
    "#     s = (np.sum(pred_train == Y_train) / len(Y_train)) * 100\n",
    "#     print(\"Bayesian Classifier Accuracy: \", s, \"%\")\n",
    "#     # train_error = utils.get_accuracy(pred_train,Y_train)\n",
    "#     # test_error =  utils.get_accuracy(pred_test,Y_test)\n",
    "#     return BayesianClassifierAccuracy, s\n",
    "\n",
    "def adaboost_classifier(Y_train, X_train, Y_test, X_test, T, clf):\n",
    "    \n",
    "    #TODO: FILL THE FUNCTION with the implementation as the steps above\n",
    "\n",
    "    # TODO [1]: Initialize weights\n",
    "    w =np.full(X_train.shape[0] , 1 / X_train.shape[0]) \n",
    "    \n",
    "    ## TODO [2]:  Initialize the training and test data with empty array placeholders\n",
    "    #### Hint: what should be their shape?\n",
    "    \n",
    "    pred_train = np.zeros(Y_train.shape) ## predicted classes of the training examples\n",
    "    pred_test = np.zeros(Y_test.shape)  ## predicted classes of the test examples\n",
    "\n",
    "    ## TODO [3]: loop over the boosting iterations \n",
    "    for i in range(T): \n",
    "\n",
    "        # TODO [4]: Fit a classifier with the specific weights \n",
    "        ## TODO [4.A]: fit the classifier on the training data\n",
    "        #### Hint: search how sklearn.tree.DecisionTreeClassifier fits classifier on data\n",
    "        ### Hint: search for parameter weights in the fit matrix\n",
    "        clf.fit(X_train , Y_train, w)\n",
    "        # TODO [4.B]: predict classes for the training data and test data\n",
    "        pred_train_i = clf.predict(X_train) \n",
    "        pred_test_i = clf.predict(X_test) \n",
    "        # TODO [5]: calculate the miss Indicator function\n",
    "        m_I = (Y_train!= pred_train_i).astype(int)\n",
    "        # TODO [6]: calculate the error for the current classifier (err_t)\n",
    "        err_t = ( np.sum(w * m_I) ) / (np.sum(w)) \n",
    "        \n",
    "        # TODO [7]: calculate current classifier weight (Alpha_t)\n",
    "        alpha_t = np.log((1 - err_t) / err_t)\n",
    "        \n",
    "        # TODO [8]: update the weights \n",
    "        w = w * np.exp(alpha_t * m_I)\n",
    "        w = w / np.sum(w)\n",
    "        # TODO [9] Add to the overall predictions\n",
    "        pred_train += alpha_t * pred_train_i\n",
    "        pred_test += alpha_t * pred_test_i\n",
    "        \n",
    "    pred_train = np.sign(pred_train)\n",
    "    pred_test = np.sign(pred_test)\n",
    "    # TODO [10]: Return error rate in train and test set\n",
    "    #### Hint: use function get_accuracy from utils.py\n",
    "    # train_error = utils.get_accuracy(pred_train,Y_train)\n",
    "    # test_error =  utils.get_accuracy(pred_test,Y_test)\n",
    "    BayesianClassifierAccuracy = (np.sum(pred_test == Y_test) / len(Y_test)) * 100\n",
    "    print(\"Bayesian Classifier Accuracy: \", BayesianClassifierAccuracy, \"%\")\n",
    "\n",
    "    s = (np.sum(pred_train == Y_train) / len(Y_train)) * 100\n",
    "    print(\"Bayesian Classifier Accuracy: \", pred_train,Y_test, \"%\")\n",
    "    return BayesianClassifierAccuracy, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(337, 1012)\n",
      "Number of Iterations :  10\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  60\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  110\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  160\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  210\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  260\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  310\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n",
      "Number of Iterations :  360\n",
      "Bayesian Classifier Accuracy:  0.0 %\n",
      "Bayesian Classifier Accuracy:  [-1. -1. -1. ... -1. -1. -1.] [4 2 1 2 6 8 7 9 4 9 4 2 7 4 2 4 9 3 8 8 5 8 6 6 5 6 2 4 7 8 5 3 6 9 5 4 4\n",
      " 4 5 1 8 1 7 5 1 6 5 2 1 2 2 4 8 3 1 1 6 2 4 1 9 1 5 7 6 6 4 3 5 6 3 5 8 4\n",
      " 5 1 7 2 4 6 9 6 4 2 9 9 7 9 1 8 7 2 5 7 3 3 4 7 5 9 1 5 4 5 9 3 1 3 2 8 9\n",
      " 5 6 6 3 9 5 6 2 1 1 5 6 8 7 5 3 4 4 7 9 3 3 3 5 5 6 2 1 5 2 4 8 8 3 1 2 3\n",
      " 6 7 7 2 5 5 8 6 5 4 6 4 1 4 3 6 3 2 8 7 5 9 2 7 7 8 4 8 5 2 5 5 5 5 4 3 4\n",
      " 7 9 8 5 2 5 6 5 4 9 6 9 8 9 3 6 8 3 8 8 9 1 4 2 6 2 9 7 6 4 3 9 3 9 4 5 7\n",
      " 3 9 6 6 1 4 1 6 5 5 2 4 3 1 7 8 7 2 4 2 5 9 2 3 1 4 6 3 8 7 9 9 4 7 9 2 5\n",
      " 2 6 8 8 7 5 2 4 1 2 7 2 5 9 6 2 1 9 4 5 1 4 6 4 1 5 8 4 7 5 5 5 7 5 5 2 2\n",
      " 4 8 3 3 2 9 4 7 2 8 2 7 2 1 1 3 9 4 7 7 2 5 6 7 3 5 9 8 6 9 1 2 1 8 9 8 3\n",
      " 1 4 8 9] %\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16092/3276442347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Compare error rate vs number of iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit a simple decision tree first\n",
    "clf_tree = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "\n",
    "# Fit Adaboost classifier using a decision tree as base estimator\n",
    "# Test with different number of iterations\n",
    "acc_train, acc_test = [],[]\n",
    "x_range = range(10, 410, 50)\n",
    "print(xTest.shape)\n",
    "for i in x_range:\n",
    "    print('Number of Iterations : ' , i)\n",
    "    acc_i = adaboost_classifier(yTrain, xTrain, yTest, xTest, i, clf_tree)\n",
    "    acc_train.append(acc_i[0])\n",
    "    acc_test.append(acc_i[1])\n",
    "\n",
    "# Compare error rate vs number of iterations\n",
    "utils.plot_accuracy(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 1, 2, 3, 1, 6, 8, 8, 1, 1, 3, 0, 0, 6, 7, 0, 4, 7, 7, 3, 7, 1, 4, 0, 5, 3, 0, 4, 0, 3, 1, 1, 7, 2, 5, 1, 0, 2, 1, 1, 0, 1, 0, 4, 4, 5, 2, 0, 0, 3, 7, 6, 1, 2, 2, 7, 1, 1, 2, 7, 7, 4, 6, 1, 1, 1, 8, 5, 1, 1, 1, 6, 5, 2, 3, 3, 8, 4, 4, 6, 0, 6, 0, 0, 8, 2, 7, 8, 2, 3, 2, 1, 1, 4, 1, 1, 1, 8, 0, 1, 0, 1, 2, 1, 4, 2, 4, 1, 1, 7, 1, 5, 8, 4, 0, 8, 7, 1, 7, 2, 5, 2, 7, 5, 3, 8, 1, 4, 8, 2, 0, 1, 3, 1, 4, 2, 8, 5, 1, 5, 8, 1, 5, 3, 1, 4, 6, 1, 2, 2, 7, 6, 0, 7, 0, 7, 1, 5, 3, 2, 5, 0, 1, 7, 0, 0, 1, 7, 1, 2, 1, 2, 5, 7, 1, 1, 0, 6, 2, 8, 5, 1, 8, 7, 3, 1, 1, 8, 2, 6, 6, 1, 7, 4, 5, 3, 3, 2, 6, 6, 2, 8, 5, 5, 3, 0, 1, 6, 3, 5, 2, 0, 7, 4, 1, 1, 6, 0, 4, 0, 5, 0, 6, 1, 0, 1, 6, 0, 6, 8, 2, 1, 1, 8, 7, 8, 0, 1, 0, 1, 5, 4, 7, 2, 2, 5, 6, 8, 0, 1, 1, 3, 7, 3, 3, 6, 5, 7, 0, 8, 2, 5, 6, 6, 1, 0, 8, 3, 8, 8, 2, 5, 3, 3, 7, 8, 1, 3, 4, 1, 6, 7, 6, 4, 1, 2, 2, 3, 1, 1, 0, 7, 2, 2, 7, 3, 2, 8, 3, 1, 4, 7, 1, 2, 7, 7, 2, 1, 0, 6, 2, 1, 1, 8, 3, 0, 8, 7, 1, 5, 4, 0, 1, 3, 1, 4, 0, 3, 2, 2, 1, 5, 4, 2, 0, 0] [4 7 3 3 7 7 7 9 9 4 2 4 1 1 4 8 1 5 5 8 7 9 2 9 1 6 7 1 9 8 7 3 2 6 3 6 3\n",
      " 1 5 2 6 1 6 1 5 5 4 8 1 8 4 8 9 2 3 3 3 2 3 3 8 8 5 7 2 5 2 9 7 6 2 4 7 4\n",
      " 3 6 5 9 9 5 7 1 4 1 1 9 8 8 9 3 6 3 4 4 5 2 3 2 9 1 2 1 2 3 2 5 3 5 4 3 8\n",
      " 4 7 9 1 1 9 8 6 8 3 6 3 8 4 4 9 4 5 9 4 5 2 5 6 5 3 9 6 8 6 9 3 4 4 6 5 7\n",
      " 2 3 4 8 7 1 3 1 6 2 4 9 3 6 1 2 8 1 1 2 8 2 9 6 3 7 8 3 2 1 7 6 5 6 2 9 6\n",
      " 4 3 2 9 4 7 7 2 5 1 8 6 4 6 9 7 3 9 6 3 6 1 2 7 7 8 1 5 8 5 8 2 5 5 9 9 8\n",
      " 4 7 4 1 4 4 1 7 9 8 3 2 9 3 9 3 6 1 6 7 5 9 3 4 8 5 9 1 6 2 7 8 7 5 7 8 8\n",
      " 1 9 3 7 9 7 4 1 9 6 5 9 1 6 7 4 9 9 3 7 9 2 7 3 3 5 2 1 5 4 2 6 1 8 3 4 8\n",
      " 6 4 9 9 8 5 6 4 3 6 3 5 2 1 7 1 2 6 9 4 1 9 8 2 4 7 5 6 6 4 5 1 9 8 4 2 4\n",
      " 1 4 5 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(minDistPredictions),yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = rgb2gray(io.imread(\"mark1.jpg\"))\n",
    "Otsu_Threshold = threshold_otsu(m1)   \n",
    "m1 = m1 < Otsu_Threshold \n",
    "\n",
    "m2 = rgb2gray(io.imread(\"mark2.jpg\"))\n",
    "Otsu_Threshold = threshold_otsu(m2)   \n",
    "m2 = m2 < Otsu_Threshold \n",
    "\n",
    "i1 = rgb2gray(io.imread(\"ACdata_base/9/1510.JPG\"))\n",
    "Otsu_Threshold = threshold_otsu(i1)   \n",
    "i1 = i1 < Otsu_Threshold \n",
    "\n",
    "img,edges,ske,text,diacritics = preprocess(\"ACdata_base/9/1510.JPG\")\n",
    "arra  = np.zeros(diacritics.shape)\n",
    "arra = diacritics == 0\n",
    "\n",
    "d1 = cv2.matchShapes(diacritics.astype(np.uint8),m2.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0)\n",
    "\n",
    "img,edges,ske,text,diacritics = preprocess(\"ACdata_base/8/1358.JPG\")\n",
    "\n",
    "d2 = cv2.matchShapes(diacritics.astype(np.uint8),m2.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0)\n",
    "\n",
    "\n",
    "print(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Classifier Accuracy:  95.84569732937686 %\n"
     ]
    }
   ],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "predict_Test = clf.predict(xTest)\n",
    "kk = (np.sum(predict_Test == yTest) / len(yTest)) * 100\n",
    "print(\"Bayesian Classifier Accuracy: \", kk, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
