{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports you will need in the whole world\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage.transform import (hough_line, hough_line_peaks)\n",
    "from skimage.feature import greycomatrix, greycoprops, canny, corner_harris\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing,skeletonize, thin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.filters import sobel_h, sobel, sobel_v,roberts, prewitt,threshold_otsu\n",
    "import glob\n",
    "import math\n",
    "from matplotlib import cm\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, titles=None):\n",
    "    # This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None:\n",
    "        titles = ['(%d)' % i for i in range(1, n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        a = fig.add_subplot(1, n_ims, n)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = np.asarray(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(gray):\n",
    "\n",
    "    # gray = np.uint8(gray)\n",
    "    img = cv2.GaussianBlur(gray,(3,3),0)\n",
    "    # img = cv2.bilateralFilter(gray,9,75,75)\n",
    "    # convolute with proper kernels\n",
    "    # img = np.uint8(img)\n",
    "    dest = cv2.Laplacian(img, cv2.CV_16S, ksize=3)\n",
    "    abs_dest = cv2.convertScaleAbs(dest)\n",
    "    # show_images([abs_dest])\n",
    "    Otsu_Threshold = threshold_otsu(img)   \n",
    "    if (img[0,0] < Otsu_Threshold and img[0,-1] < Otsu_Threshold and img[-1,0] < Otsu_Threshold and img[-1,0] < Otsu_Threshold):\n",
    "        binary = img > Otsu_Threshold         \n",
    "    else:\n",
    "        binary = img < Otsu_Threshold  \n",
    "    skeleton_img =  skeletonize(binary)\n",
    "\n",
    "    #separating the diac.\n",
    "    H = np.sum(binary,axis = 1)\n",
    "    I = np.argmax(H)\n",
    "    point = [(x,I) for x in range(binary.shape[0])]\n",
    "    # print(I)\n",
    "    h, w = binary.shape[:2]\n",
    "    # print(h,w)\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "    # print(mask)\n",
    "    binary = binary < 1 \n",
    "    binary = binary.astype(np.uint8)\n",
    "    # show_images([binary])\n",
    "    for j in range(binary.shape[1] - 1):\n",
    "        if binary[I][j] == 0 and binary[I][j+1] == 1:\n",
    "            cv2.floodFill(binary,mask,seedPoint = (j,I),newVal = 1) \n",
    "    # print(b)\n",
    "    text = mask\n",
    "    diacritics = binary\n",
    "    return img,abs_dest,skeleton_img,text,diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HVSL(edge):\n",
    "    horizontal = edge.copy()\n",
    "    vertical = edge.copy()\n",
    "    H = 0\n",
    "    V = 0 \n",
    "    \n",
    "    cols = horizontal.shape[1]\n",
    "    horizontal_size = cols / 30\n",
    "    horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (int(horizontal_size + 4), 1))\n",
    "    horizontal = cv2.erode(horizontal, horizontalStructure)\n",
    "    horizontal = cv2.dilate(horizontal, horizontalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(horizontal)   \n",
    "    horizontal = horizontal > Otsu_Threshold \n",
    "    H, output, stats, centroids = cv2.connectedComponentsWithStats(horizontal.astype(np.uint8),connectivity=8)\n",
    "    \n",
    "\n",
    "\n",
    "    rows = vertical.shape[0]\n",
    "    verticalsize = rows / 30\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1,int(verticalsize + 4)))\n",
    "    vertical = cv2.erode(vertical, verticalStructure)\n",
    "    vertical = cv2.dilate(vertical.astype(np.uint8), verticalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(vertical)   \n",
    "    vertical = vertical > Otsu_Threshold \n",
    "\n",
    "    V, output, stats, centroids = cv2.connectedComponentsWithStats(vertical.astype(np.uint8),connectivity=8)\n",
    "    return H - 1,V - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOE(edge):\n",
    "    houghSpace,angles, dists = hough_line(edge)\n",
    "    houghSpace,angles, dists = hough_line_peaks(houghSpace,angles,dists,threshold=0.5*np.amax(houghSpace))\n",
    "    start = -2\n",
    "    bin = []\n",
    "    while True :\n",
    "        if(start >= 2):\n",
    "            break\n",
    "        bin.append(start)\n",
    "        start += 0.01\n",
    "    bin = [round(bins,2) for bins in bin ]\n",
    "    avg_angle = np.average(angles)\n",
    "    angles = [round(angle,2) for angle in angles ]\n",
    "    hist , bins = np.histogram(angles,bins = bin)\n",
    "    return hist,bins,avg_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TOS(skeleton):\n",
    "    houghSpace,angles, dists = hough_line(skeleton)\n",
    "    houghSpace,angles, dists = hough_line_peaks(houghSpace,angles,dists,threshold=0.5*np.amax(houghSpace))\n",
    "\n",
    "    start = -2\n",
    "    bin = []\n",
    "    while True :\n",
    "        if(start >= 2):\n",
    "            break\n",
    "        bin.append(start)\n",
    "        start += 0.01\n",
    "    bin = [round(bins,2) for bins in bin ]\n",
    "    avg_angle = np.average(angles)\n",
    "    angles = [round(angle,2) for angle in angles ]\n",
    "    hist , bins = np.histogram(angles,bins = bin)\n",
    "    return hist,bins,avg_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVL(ske):\n",
    "    vertical = ske.copy()\n",
    "    V = 0 \n",
    "    rows = vertical.shape[0]\n",
    "    verticalsize = rows // 30\n",
    "    verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, int(verticalsize + 4)))\n",
    "    vertical = cv2.erode(vertical.astype(np.uint8), verticalStructure)\n",
    "    vertical = cv2.dilate(vertical.astype(np.uint8), verticalStructure)\n",
    "    Otsu_Threshold = threshold_otsu(vertical)   \n",
    "    vertical = vertical > Otsu_Threshold \n",
    "    V, output, stats, centroids = cv2.connectedComponentsWithStats(vertical.astype(np.uint8),connectivity=8)\n",
    "    sizes = stats[1:, -1]\n",
    "\n",
    "    Otsu_Threshold = threshold_otsu(ske)   \n",
    "    ske = ske > Otsu_Threshold \n",
    "    min = math.inf\n",
    "    max = -1\n",
    "    for i in range(ske.shape[0]):\n",
    "        for j in range(ske.shape[1]):\n",
    "            if (ske[i][j] == 1):\n",
    "                if i > max:\n",
    "                    max = i\n",
    "                elif i < min:\n",
    "                    min = i\n",
    "    text_hight = max - min\n",
    "    num_VL = V - 1\n",
    "    if(len(sizes) == 0):\n",
    "        sizes = [2,1,2]\n",
    "    higtest_VL = np.max(sizes)\n",
    "    drvt = abs(text_hight - higtest_VL)\n",
    "    # variance = np.var(sizes)\n",
    "    # print(sizes)\n",
    "    # print(text_hight,\"text hight\")\n",
    "    # print(num_VL,\"num_VL\")\n",
    "    # print(higtest_VL,\"higtest_VL\")\n",
    "    # print(drvt,\"drvt\")\n",
    "    # print(variance,\"variance\")\n",
    "    return text_hight,num_VL,higtest_VL,drvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tth(edge,ske):\n",
    "    Otsu_Threshold = threshold_otsu(ske)   \n",
    "    ske = ske > Otsu_Threshold \n",
    "\n",
    "    Otsu_Threshold = threshold_otsu(edge)   \n",
    "    edge = edge > Otsu_Threshold \n",
    "    \n",
    "\n",
    "    dest_up = []\n",
    "    dest_down = []\n",
    "    for i in range(ske.shape[0]):\n",
    "        for j in range(ske.shape[1]):\n",
    "            if (ske[i][j] == 1):\n",
    "                c = True\n",
    "                for k in range(i + 1,edge.shape[0]):\n",
    "                    if(edge[k][j] == 1):\n",
    "                        c = False\n",
    "                        d = abs(k - i)\n",
    "                        dest_down.append(d)\n",
    "                        break\n",
    "                if(c):\n",
    "                    dest_down.append(0)        \n",
    "                \n",
    "                u = 0\n",
    "                for k in range(0, i):\n",
    "                    if(edge[k][j] == 1):\n",
    "                        u = k\n",
    "                dest_up.append(abs(u-i))\n",
    "    return dest_down + dest_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDs(diacritics):\n",
    "\n",
    "    arra  = np.zeros(diacritics.shape)\n",
    "    arra = diacritics == 0\n",
    "\n",
    "    contours = find_contours(arra ,level = 0.2,fully_connected='high')\n",
    "\n",
    "    \n",
    "    m1 = rgb2gray(io.imread(\"mark1.jpg\"))\n",
    "    Otsu_Threshold = threshold_otsu(m1)   \n",
    "    m1 = m1 < Otsu_Threshold \n",
    "\n",
    "\n",
    "    m2 = rgb2gray(io.imread(\"mark2.jpg\"))\n",
    "    Otsu_Threshold = threshold_otsu(m2)   \n",
    "    m2 = m2 < Otsu_Threshold \n",
    "\n",
    "    dist_1 = []\n",
    "    dist_2 = []\n",
    "    bounding_boxes = []\n",
    "    for contour in contours:\n",
    "        Y_Values = np.asarray(contour[:,0])\n",
    "        X_Values = np.asarray(contour[:,1])\n",
    "        bounding_boxes.append([\n",
    "        int(np.amin(X_Values)),\n",
    "        int(np.amax(X_Values)),\n",
    "        int(np.amin(Y_Values)),\n",
    "        int(np.amax(Y_Values))])\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        [Xmin, Xmax, Ymin, Ymax] = box\n",
    "        dist_1.append(cv2.matchShapes(arra[Ymin:Ymax,Xmin:Xmax].astype(np.uint8),m1.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0))\n",
    "        dist_2.append(cv2.matchShapes(arra[Ymin:Ymax,Xmin:Xmax].astype(np.uint8),m2.astype(np.uint8),cv2.CONTOURS_MATCH_I2,0))\n",
    "    if (len(dist_1) == 0 ):\n",
    "        return 20 , 20 \n",
    "    return np.min(dist_1),np.min(dist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WOR(text):\n",
    "    contours, _ = cv2.findContours(text.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    angles = []\n",
    "    for i, c in enumerate(contours):\n",
    "        \n",
    "        # Calculate the area of each contour\n",
    "        area = cv2.contourArea(c)\n",
    "        if( area < 0.002*(len(text)*len(text[0]) or area > 0.4*(len(text)*len(text[0])))):\n",
    "            continue\n",
    "        # Ignore contours that are too small or too large\n",
    "        rect = cv2.minAreaRect(c)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "\n",
    "        center = (int(rect[0][0]),int(rect[0][1])) \n",
    "        width = int(rect[1][0])\n",
    "        height = int(rect[1][1])\n",
    "        angle = int(rect[2])\n",
    "        \n",
    "        if width < height:\n",
    "            angle = 90 - angle\n",
    "        else:\n",
    "            angle = -angle\n",
    "        angles.append(angle)\n",
    "      \n",
    "    angles = np.sort(angles)\n",
    "    for i in range(len(angles)):\n",
    "        if angles[i] == 180:\n",
    "            angles[i] = 0\n",
    "    ang = range(90)\n",
    "    hist , bins = np.histogram(angles,bins = ang)\n",
    "    ori = np.average(angles)\n",
    "    return ori,hist,len(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPP(img):\n",
    "    H = np.sum(img[1:img.shape[0]-1,:],axis = 1)\n",
    "    I = np.argmax(H)\n",
    "    H = np.sort(H)\n",
    "    if(I == 0):\n",
    "        I = 1\n",
    "    hpp = np.sum(img[:,1:img.shape[1] - 1]) / I\n",
    "    horizontal_projection = np.sum(img, axis = 1) \n",
    "    return H[-3:],hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    xData = []\n",
    "    yData = []\n",
    "    for filename in sorted(glob.glob('ACdata_base/1/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(1)\n",
    "    for filename in sorted(glob.glob('ACdata_base/2/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(2)\n",
    "    for filename in sorted(glob.glob('ACdata_base/3/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(3)\n",
    "    for filename in sorted(glob.glob('ACdata_base/4/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(4)\n",
    "    for filename in sorted(glob.glob('ACdata_base/5/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(5)\n",
    "    for filename in sorted(glob.glob('ACdata_base/6/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(6)\n",
    "    for filename in sorted(glob.glob('ACdata_base/7/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(7)\n",
    "    for filename in sorted(glob.glob('ACdata_base/8/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(8)\n",
    "    for filename in sorted(glob.glob('ACdata_base/9/*.jpg')):\n",
    "        img = cv2.imread(filename)\n",
    "        img = preprocess(img)\n",
    "        xData.append(img)\n",
    "        yData.append(9)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(np.asarray(xData), np.asarray(yData), test_size = 0.2, shuffle = True)\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featur_Extraction(xTrain):\n",
    "    Features = []\n",
    "    for gray in xTrain:\n",
    "        img_feature = []\n",
    "        img,edges,ske,text,diacritics = preprocessing(gray)\n",
    "        Otsu_Threshold = threshold_otsu(img)   \n",
    "        img = img < Otsu_Threshold\n",
    "\n",
    "        H,V = HVSL(edges)\n",
    "        img_feature.append(H)\n",
    "        img_feature.append(V)\n",
    "\n",
    "        hist_e , b_e,avg_angle_e = TOE(edges)\n",
    "        hist_s , b_s,avg_angle_s = TOS(ske)\n",
    "        img_feature.append(avg_angle_s)\n",
    "        img_feature.append(avg_angle_e)\n",
    "        # for i in range(len(hist_e)):\n",
    "        #     img_feature.append(hist_e[i])\n",
    "        #     img_feature.append(hist_s[i])\n",
    "        \n",
    "        text_hight,num_VL,higtest_VL,drvt = LVL(ske)\n",
    "        img_feature.append(text_hight)\n",
    "        img_feature.append(num_VL)\n",
    "        img_feature.append(higtest_VL)\n",
    "        img_feature.append(drvt)\n",
    "\n",
    "\n",
    "        Thickness = Tth(edges,ske)\n",
    "        avg_thick = np.average(Thickness)\n",
    "        img_feature.append(avg_thick)\n",
    "        # for i in range(300):\n",
    "        #     img_feature.append(Thickness[i])\n",
    "        \n",
    "        d1 , d2 = SDs(diacritics)\n",
    "        img_feature.append(d1)\n",
    "        img_feature.append(d2)\n",
    "        \n",
    "        H , hpp = HPP(text)\n",
    "        img_feature.append(H[0])\n",
    "        img_feature.append(H[1])\n",
    "        img_feature.append(H[2])\n",
    "        img_feature.append(hpp)\n",
    "         \n",
    "        ori,hist_o,o = WOR(text)\n",
    "        img_feature.append(ori)\n",
    "        for i in range(len(hist_o)):\n",
    "            img_feature.append(hist_o[i])\n",
    "        img_feature.append(o)\n",
    "        Features.append(img_feature)\n",
    "        \n",
    "    return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featur_Extraction_test(gray):\n",
    "    img_feature = []\n",
    "    img,edges,ske,text,diacritics = preprocessing(gray)\n",
    "    Otsu_Threshold = threshold_otsu(img)   \n",
    "    img = img < Otsu_Threshold\n",
    "\n",
    "    H,V = HVSL(edges)\n",
    "    img_feature.append(H)\n",
    "    img_feature.append(V)\n",
    "\n",
    "    hist_e , b_e,avg_angle_e = TOE(edges)\n",
    "    hist_s , b_s,avg_angle_s = TOS(ske)\n",
    "    img_feature.append(avg_angle_s)\n",
    "    img_feature.append(avg_angle_e)\n",
    "    \n",
    "    text_hight,num_VL,higtest_VL,drvt = LVL(ske)\n",
    "    img_feature.append(text_hight)\n",
    "    img_feature.append(num_VL)\n",
    "    img_feature.append(higtest_VL)\n",
    "    img_feature.append(drvt)\n",
    "\n",
    "\n",
    "    Thickness = Tth(edges,ske)\n",
    "    avg_thickness = np.average(Thickness)\n",
    "    img_feature.append(avg_thickness)\n",
    "#     for i in range(300):\n",
    "#             img_feature.append(Thickness[i])\n",
    "    \n",
    "    d1 , d2 = SDs(diacritics)\n",
    "    img_feature.append(d1)\n",
    "    img_feature.append(d2)\n",
    "    \n",
    "    H , hpp = HPP(text)\n",
    "    # diff = abs((H[0] - H[1]) + (H[1] - H[2]))\n",
    "    img_feature.append(H[0])\n",
    "    img_feature.append(H[1])\n",
    "    img_feature.append(H[2])\n",
    "    img_feature.append(hpp)\n",
    "        \n",
    "    ori,hist_o,o = WOR(text)\n",
    "    img_feature.append(ori)\n",
    "    for i in range(len(hist_o)):\n",
    "        img_feature.append(hist_o[i])\n",
    "    img_feature.append(o)\n",
    "    return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.cvtColor(cv2.imread(\"ACdata_base/9/1503.jpg\"), cv2.COLOR_RGB2GRAY)\n",
    "# img,edges,ske,text,diacritics = preprocessing(img)\n",
    "# show_images([img,edges,ske,text,diacritics])\n",
    "# H,V = HVSL(edges)\n",
    "# hist_e,b_e,avg_angle_e = TOE(edges)\n",
    "# hist_s,b_s,avg_angle_s = TOS(ske)\n",
    "# text_hight,num_VL,higtest_VL,drvt = LVL(ske)\n",
    "# thickness = Tth(edges,ske)\n",
    "# d1 , d2 =SDs(diacritics)\n",
    "# HH,hpp = HPP(text)\n",
    "# ori,hist_o,o = WOR(text)\n",
    "# print([H,V,avg_angle_e,avg_angle_e,text_hight,num_VL,higtest_VL,drvt,np.average(thickness),d1,d2,HH,hpp,ori])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashra\\AppData\\Local\\Temp/ipykernel_1956/2948043143.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  xTrain, xTest, yTrain, yTest = train_test_split(np.asarray(xData), np.asarray(yData), test_size = 0.2, shuffle = True)\n",
      "<__array_function__ internals>:5: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = readData()\n",
    "xTrain = Featur_Extraction(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "print(len(xTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest = np.asarray(xTest)\n",
    "xTrain = np.asarray(xTrain)\n",
    "yTest = np.asarray(yTest)\n",
    "yTrain = np.asarray(yTrain)\n",
    "\n",
    "x_test = []\n",
    "for i in range(xTest.shape[0]):\n",
    "    testPoint = np.asarray(Featur_Extraction_test(xTest[i]))\n",
    "    x_test.append(testPoint)\n",
    "# x_test = np.asarray(x_test)\n",
    "\n",
    "# for i in range(x_test.shape[0]):\n",
    "#     for j in range(x_test.shape[1]):\n",
    "#         if(x_test[i][j] > 10000 ):\n",
    "#             x_test[i][j] = 0\n",
    "xTest = np.asarray(x_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(xTrain.shape[0]):\n",
    "    for j in range(xTrain.shape[1]):\n",
    "        if(xTrain[i][j] > 10000 or math.isnan (xTrain[i][j])):\n",
    "            xTrain[i][j] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(xTest.shape[0]):\n",
    "    for j in range(xTest.shape[1]):\n",
    "        if(xTest[i][j] > 10000 or math.isnan (xTest[i][j])):\n",
    "            xTest[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:  83.67952522255193 %\n"
     ]
    }
   ],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "predict_Test = clf.predict(xTest)\n",
    "SVM_pred = (np.sum(predict_Test == yTest) / len(yTest)) * 100\n",
    "print(\"SVM: \", SVM_pred, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP:  86.05341246290801 %\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
    "classifier.fit(xTrain, yTrain)\n",
    "MLP_pred = classifier.predict(xTest)\n",
    "kk = (np.sum(MLP_pred == yTest) / len(yTest)) * 100\n",
    "print(\"MLP: \", kk, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:  68.84272997032642 %\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(xTrain, yTrain)\n",
    "KNN_p = knn.predict(xTest)\n",
    "KNN = (np.sum(KNN_p == yTest) / len(yTest)) * 100\n",
    "print(\"KNN: \", KNN, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gau:  56.083086053412465 %\n"
     ]
    }
   ],
   "source": [
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(xTrain,yTrain)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict(xTest) # 0:Overcast, 2:Mild\n",
    "gau = (np.sum(predicted == yTest) / len(yTest)) * 100\n",
    "print(\"gau: \", gau, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:  64.09495548961425 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "classifier.fit(xTrain, yTrain)\n",
    "y_pred = classifier.predict(xTest)\n",
    "RF = (np.sum(y_pred == yTest) / len(yTest)) * 100\n",
    "print(\"RF: \", RF, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adb:  54.3026706231454 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate = 0.01)\n",
    "clf.fit(xTrain, yTrain)\n",
    "adb_p = clf.predict(xTest)\n",
    "adb = (np.sum(adb_p == yTest) / len(yTest)) * 100\n",
    "print(\"adb: \", adb, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:  33.531157270029674 %\n"
     ]
    }
   ],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='poly') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "predict_Test = clf.predict(xTest)\n",
    "SVM_pred = (np.sum(predict_Test == yTest) / len(yTest)) * 100\n",
    "print(\"SVM: \", SVM_pred, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:  57.566765578635014 %\n"
     ]
    }
   ],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "predict_Test = clf.predict(xTest)\n",
    "SVM_pred = (np.sum(predict_Test == yTest) / len(yTest)) * 100\n",
    "print(\"SVM: \", SVM_pred, \"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
